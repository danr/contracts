Haskell programmers enjoy the benefits of strong static types and purity: 
static types eliminate many bugs early on in the development cycle, and purity 
simplifies equational reasoning about programs. Despite these benefits, however, 
bugs may still remain inside purely functional code and programs often
crash if applied to the wrong arguments. 

Consider this Haskell definition:
\begin{code}
  f xs = head (reverse (True : xs))
  g xs = head (reverse xs)
\end{code}
Both @f@ and @g@ are well typed (and hence do not ``go wrong'' in Milner's 
sense), but @g@ can crash (when applied to the empty list), whereas @f@ cannot.
To distinguish the two we need reasoning that goes well beyond 
that typically embodied in a standard type system.

Many variations of {\em dependent type systems}~\cite{norell:thesis,Xi:2007:DMA:1230756.1230759,fstar} or 
{\em refinement type systems}~\cite{Rondon:2008:LT:1375581.1375602,Knowles+:sage}
have been proposed to address this problem, each offering different degrees of 
expressiveness or automation. 
Another line of work aiming to address this problem, studied by many researchers
as well~\cite{Findler:2002:CHF:581478.581484,Blume:2006:SCM:1166013.1166016,Knowles+:sage,Siek06gradualtyping,Wadler:2009:WPC:1532974.1532976}, allows programmers to annotate 
functions with {\em contracts}, which are forms of behavioural specifications.
For instance, we might write the following contract for 
@reverse@: 
\[ @reverse@ \in (xs : \CF) -> \{ ys \mid @null@\;xs\; @<=>@\; @null@\;ys \}  \] 
%% \begin{code}
%%   reverse ::: xs:CF -> { ys | null xs <=> null ys }
%% \end{code}
This contract annotation asserts that if @reverse@ is applied to a 
crash-free (@CF@) argument list @xs@ then the result @ys@ will be empty (@null@) 
if and only if @xs@ is empty. What is a crash-free argument? 
Since we are using lazy semantics, a list could possibly contain cons-cells that yield 
errors only when evaluated, and the @CF@ precondition asserts that the input list 
is not one of those.

Notice also that @null@ and @<=>@ are just ordinary Haskell functions, perhaps
written by the programmer, even though they appear inside contracts.
With this in hand we might hope to prove that @f@ satisfies the contract
\[ @f@ \in \CF -> \CF \] 
But how do we verify that @reverse@ and @f@ satisfy the claimed
contracts? Contracts are often tested dynamically, but 
our plan here is different: we want to verify contracts \emph{statically} 
and \emph{automatically}. 

It should be clear that there is a good deal of logical reasoning to do,
and a now-popular approach is to hand off the task to an off-the-shelf theorem
prover such as Z3~\cite{z3citation} or Vampire~\cite{vampire}, or search for 
counterexamples with a finite model finder~\cite{paradox}.
With that in mind, we make the following new contributions:

\begin{itemize}
  \item We give a translation of Haskell programs to first-order logic (FOL) theories. 
        It turns out that lazy programs (as opposed to
        strict programs!) have a very natural translation into first-order logic
        (Section~\ref{ssect:trans-fol}).
  \item We give a translation of contracts to FOL formulae, and an axiomatisation of 
        the language semantics in FOL 
        (Section~\ref{s:contracts-fol}).
  \item Our main contribution is to show that if we can prove the formula 
        that arises from a contract translation 
        for a given program, then the program does indeed satisfy this contract. Our proof
        uses the novel idea of employing the denotational 
        semantics as a first-order model (Section~\ref{ssect:denot}).
  \item We show how to use this translation in practice for static contract checking with
        a SAT-solver (Section~\ref{sect:soundness}), 
        and how to prove goals by induction (Section~\ref{sect:extensions}).
\end{itemize}

We consider this work to be the first step towards practical contract checking 
for Haskell programs, that lays out the theoretical foundations for further engineering 
and experimentation. Nevertheless, we have already implemented a prototype for Haskell 
programs that uses GHC as a front-end. We have evaluated the practicality of our approach 
on many examples, including lazy and higher-order programs, and goals that require 
induction. We report this initial encouraging evaluation in 
Section~\ref{sect:implementation}. 

To our knowledge no-one has previously presented a translation of lazy higher-order programs to 
first-order logic in a provably sound way with respect to a denotational
semantics. Furthermore, our approach to static contract checking is 
distinctly different to previous work: instead of wrapping and 
symbolic execution~\cite{xu+:contracts,Xu:2012:HCC:2103746.2103767}, 
we harness purity and laziness to directly use the denotational semantics
of programs and contracts and discharge the obligations with a SAT-solver, side-stepping
the wrapping process. Instead of verification condition generation by pushing
pre- and post- conditions through a program, we directly ask a theorem prover to prove 
a contract for the FOL encoding of a program. 
We further discuss similarities and differences compared to related work in Section~\ref{sect:related}.

%% \newpage 
%%   \item The translation
%% For this paper we focus on the translation, but to substantiate the practicality 
        
        
%% \end{itemize} 


%% \begin{itemize}
%% \item We show how to translate Haskell terms
%% into First Order Logic (FOL) (Section~\ref{ssect:trans-fol}).  
%% It may appear surprising that this 
%% is even possible, since Haskell is a higher order language.  Although
%% the basic idea of the translation is folklore in the community,
%% we believe that this paper is the first to explain it explicitly.

%% \item We also show how to translate \emph{contracts} into FOL
%%       (Section~\ref{s:contracts-fol}), 
%%       a translation that is rather less obvious.

%% \item We give a proof based on denotational semantics 
%% that if the FOL prover discharges a 
%% suitable theorem about the translated Haskell term and contract, 
%% then indeed the original Haskell term satisfies that contract (Section~\ref{s:xxx}).

%% %\item It is one thing to make a sound translation, and quite another
%% %to produce FOL terms that the FOL prover can actually prove anything
%% %about --- a common experience is that it goes out to lunch instead.  We
%% %describe a number of techniques that dramatically improve
%% %theorem-proving times, moving them from infeasible to feasible (Section\ref{xxx}).

%% \item For this paper we focus on the
%% translation, but we have also implemented a static contract checker
%% for Haskell itself, by using GHC as a front end.  We have evaluated
%% the practicality of this approach on many examples, including lazy and
%% higher-order programs, as we describe in Section~\ref{xxx}.  \spj{I'd like
%% to say something more substantial here.}
%% \end{itemize}



















